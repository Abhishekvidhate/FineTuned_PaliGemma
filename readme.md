# Fine Tuning Multi-Modal Language Model ( VLM )

for this assignment I selected **Pali Gemma** model to work on. Pali Gemma is an advanced open-source vision-language model by Google Research, designed for tasks like image captioning and visual question answering
I selected **Pali Gemma because of it's **Architecture**

![image](https://github.com/Abhishekvidhate/FineTuned_PaliGemma/assets/120262589/8bf94eda-6f61-42d6-8cb3-0626f5c8d275)

- **Advanced Architecture**: Utilizes cutting-edge transformer techniques.
- **Extensive Training Data**: Trained on large, diverse datasets.
- **Superior Performance**: Achieves top results in benchmarks.
- **Efficiency**: Optimized for computational efficiency.
- **Versatility**: Effective across a wide range of vision-language tasks.
- **Strong Community**: Supported by Google Research, ensuring continuous updates and improvements.
